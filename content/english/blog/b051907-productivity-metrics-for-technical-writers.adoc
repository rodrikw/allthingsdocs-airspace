---
title: "Productivity metrics for technical writers"
date: 2022-05-30T10:58:20+02:00
author: "Rodrik Wade"
image: "images/blog/dashboard.jpg"
image_caption: "<a href='https://unsplash.com/photos/MHIw0nSxCR4'>Assorted dial-meter board</a> by <a href='https://unsplash.com/@condorito1953'>Arie Wubben</a>, <a href='https://unsplash.com/license'>Unsplash licence</a>, cropped"
bg_image: "images/feature-bg.jpg"
categories: ["Managerial"]
tags: ["Metrics","Productivity"]
description: "A discussion of productivity metrics that can be applied to technical writers"
draft: false
type: "post"
---

On a number of occasions I've been asked to suggest a productivity metric for technical writers, usually as part of a performance management initiative.

When it comes to metrics, I'm more in the "`Not everything that counts can be counted`" camp than in the "`If it can't be measured, it can't be managed`" one.
But if you are required to devise a metric, you owe it to your team and company to come up with something that is workable and fair.

At its simplest, productivity is the rate at which work is done:

....
Work ÷ Time = Productivity
....

But this isn't very helpful.
Obviously, churning out reams of pointless documentation like https://dilbert.com/strip/2007-11-28[Tina the tech writer^] isn't useful productivity.

In his blog https://idratherbewriting.com/2012/03/02/technical-communication-metrics-what-should-you-track/[Technical Communication Metrics: What Should You Track?^] Tom Johnson quotes Jack Barr and Stephanie Rosenbaum, who define productivity as{empty}footnote:[Barr, J.P. and Rosenbaum, S. (2003). _Documentation and Training Productivity Benchmarks_, Volume 50, No. 4, Nov 2003. p471. https://www.thefreelibrary.com/Documentation+and+training+productivity+benchmarks.-a0111165965[Online^].]:

....
(Quality × Quantity) ÷ Time = Productivity
....

This is an improvement, but it's still missing something.
We need to take into account that not all (tech doc) work is equally difficult:
writing up a procedure for a consumer application is easier than explaining the workings of a complex fraud detection algorithm.

So at the very least we need to add complexity to the formula:

....
(Quality × Quantity × Complexity) ÷ Time = Productivity
....

Now we need to find reliable measures for each of the four variables on the left.

== Time

Time is relatively easy, provided your team members are already logging their time.
If not, you might be able to get by with estimates.

[#_Complexity]
== Complexity

Complexity can be incorporated by agreeing on a set of weightings for different document types (or work products):

[cols="n,>",width=50%,frame=ends]
|===
|Work product | Complexity weighting

|End user guide
|1.0

|Administrator guide
|1.5

|Developer guide
|2.0

|===

== Quantity

Measuring quantity is where developing a workable productivity metric starts to get tricky.
You need to measure something that represents the bulk of the work being done, and it should be independent of time.

One way to measure _raw_ quantity in technical documentation is to count the number of words that technical writers write in a given period.
Word counts have a bad reputation because they can perversely incentivize verbosity and disincentivize other productive, even essential activities.
For example, technical writers might be discouraged from considering other, possibly more effective, approaches to conveying information, such as diagrams and images.
Even more importantly, prioritizing word counts could discourage planning and design, which according to JoAnn Hackos{empty}footnote:[Hackos, J.T. (2007). _Information Development: Managing Your Documentation Projects, Portfolio, and People_, (2nd ed.). Wiley. p334.]  should account for 30 percent of a documentation project's time.

How can these perverse effects be countered?

One option is to add additional quantity variables relating to important work products.
But this is likely to be impractical: even if you can arrive at a weighting for diagrams versus words, how would you take into account the value of planning, especially planning that obviates unnecessary work?

An alternative, is to rely on the quality variable.
The idea is that if you have a valid measure of quality, you can ensure that those aspects of the technical writers`' job that are not directly measured are nevertheless accounted for through their impact on quality.
Not including diagrams where they would be helpful, will drag down the quality rating.
Conversely, adequate time spent on planning and design should be reflected in an increased quality score.
This, however, makes measuring quality accurately even more critical.

== Quality

Being able to put the `(Quality × Quantity × Complexity) ÷ Time = Productivity` formula into operation, heavily depends on devising a reliable measure of quality.

One way is to ask expert reviewers and editors to provide a quality rating for each new document or update.
This can then be incorporated into the productivity calculation:

.Productivity points calculated using a quality rating.
image::{imgpath}b051907-simple-productivity-spreadsheet.png[]

The formula used to calculate productivity points (column E) is:

[source,excel]
----
=@WordCount*@Complexity*@QualityRating*QualityMultiplier
----

* `WordCount` is the number of words added to the document.

* `Complexity` is a scale like the one <<_Complexity,proposed above>>.

* `QualityRating` is a value on a five-point scale from `0` (Unacceptable) to `4` (Excellent).
This is essentially the same as the scale employed in _Developing Quality Technical information_.{empty}footnote:[Carey, M., Lanyi, M.M., Longo, D., Radzinski, E., Rouiller, S. and Wilde, E. (2014). _Developing Quality Technical Information: A Handbook for Writers and Editors_. IBM Press. p545.]

* `PointsMultiplier` is simply used to adjust the output of the formula to convenient values.

Productivity points do not have time as a denominator, unlike productivity (column F).
This means that you don't actually need to track the time for individual tasks.
Instead, you could simply set team members a target for a defined period such as a 40-hour week.
For example, specifying a `PointsMultiplier` value of `0.5`, you might set team members a minimum target of 5000 productivity points per week, equivalent to 1000 words per eight-hour day, of average complexity (`1`) and average quality (`2`).

Individual team members might meet this minimum target in a variety of ways:

One writer might work on developer documents assigned a complexity of `2` and so need to write only 500 words a day.
Another writer might consistently achieve a quality score of `3` and so require only about 670 words a day.
And a third, relatively weak, writer might find that they have to churn out 2000 words a day to make the target because their writing is consistently assigned a quality rating of only `1`.

The formula could be refined.
For example, you might distinguish content-level quality (accuracy, completeness, and relevance) from form-level quality (structure, language, and style).
The former would be assessed by the subject-matter expert and the latter by an editor.
You could even make use of a document review checklist such as the one provided in _Developing Quality Technical information_.{empty}footnote:[Ibid.]

.Pros and cons of using a quality rating
[cols="a,a",frame=ends]
|===
|Pros |Cons

|
* Requires managing only a few inputs.

* Can be adjusted to balance quality and quantity, and so avoid perverse effects.

* Should be resistant to attempts by writers to manipulate the system.

* Does not require tracking time per task.

|
* Requires gaining the cooperation of the editors and expert reviewers, which may be impractical.

* Might be susceptible to how generously different reviewers rate quality, leading to issues of unfairness.

* Doesn't account for other documentation-related work such as editing, restructuring and so on.

|===

An alternative to using an explicit quality rating is to find a reasonable proxy for it.
I believe that this can be done using the ratio between the time spent by the writer creating or updating a document, and the time spent reviewing and editing the document.
The idea is that writers who are producing good quality work will consistently require _relatively_ less time from reviewers and editors.
Weaker writers require more time on the part of subject-matter experts to check and correct content, and from editors who need to perform structural editing.

.Productivity points calculated using the review–write ratio.
image::{imgpath}b051907-writer-reviewer-ratio-spreadsheet.png[]

The formula for calculating productivity points is now:

[source,excel]
----
=(@WordCount*@Complexity+(@WordCount*-((@ReviewerTime/@WriterTime)-BaselineRatio)*QualityMultiplier))
----

* `WriterTime` is the total amount of time logged by the technical writer in creating or updating the document.

* `ReviewerTime` is the total amount of time logged by reviewers and editors of the document.

* `BaselineRatio` is the ratio of reviewer time to writer time that neither increases nor decreases the product of `WordCount` × `Complexity`.

* `QualityMultiplier` is used to weight the effect of the reviewer–writer ratio on the overall productivity score.

As with previous approach, it is possible to arrive at a minimum weekly target of 5000 productivity points.
As depicted above, with the baseline ratio specified as `1/8` and the quality multiplier set to `6`, this equivalent to 1000 words of average complexity (`1`) per eight-hour day, requiring 1 hour of review time.

If a tech writer spends 8 hours writing 1000 words, but the reviewers require only 0.5 hours, then the tech writer will earn 1375 productivity points (row 3):

.Effect of reducing the reviewer time.
image::{imgpath}b051907-writer-reviewer-ratio-spreadsheet-2.png[]

The rationale is that the technical writer has produced a higher quality product requiring less remedial input.

But what happens if the technical writer takes longer on the task without a reduction in the reviewer time?
Consider a case where a technical writer takes twice as long to complete 1000 words (row 3):

.Effect of increasing the writer time.
image::{imgpath}b051907-writer-reviewer-ratio-spreadsheet-3.png[]

The writer would again earn 1375 productivity points instead of 1000.
However, notice that they _are_ earning points at a slower rate, so that within the 40-hour week they will earn a total of only 4375 productivity points (all other tasks being equal).
Nevertheless, it still seems a little counter-intuitive for the writer to receive more productivity points for the same output.

In order to arrive at an output that better fitted my expectations, I included the WordCount–WriterTime ratio in the formula:

image::{imgpath}b051907-productivity-eqn-2.svg[width=100%]

Or as an Excel formula:

[source,excel]
----
=(((@WordCount/@WriterTime)/(BaselineRatio*TargetDailyWords))-(((@ReviewerTime/@WriterTime)-BaselineRatio)*QualityMultiplier))*PointsMultiplier
----

* `TargetDailyWord` specifies the number of words that technical writers are expected to produce per day.

* `PointsMultiplier` is used to adjust the output of the formula to convenient values.
If the `PointsMultiplier` equals `TargetDailyWords`, `Complexity` is `1` and the review–write ratio is equal to the specified `BaselineRatio`, each word will result in one point.

Fundamentally, the formula subtracts the ReviewerTime–WriterTime ratio from the WordCount–WriterTime ratio.
The effect is that changes to the reviewer time influence the score as before, but changes to the WriterTime are "`counteracted`" by the WordCount–WriterTime ratio.
The image below shows how this works:

.Comparison of the outputs of the original and revised formulas.
image::{imgpath}b051907-writer-reviewer-ratio-spreadsheet-4.png[]

Notice that in row 3, as the review time decreases, the productivity points resulting from the original formula (column J) and the revised formula (column P) both increase.
However, when in row 4 the writer time is reduced, the resulting productivity points are different.
Using the revised formula, the productivity points increase.
I believe this is more intuitive: the writer is producing the same amount of work in less time while requiring no greater effort by the reviewers (implying constant quality), so is more productive.

Similarly, in row 5 when the writer takes longer to do the work while requiring the same amount of effort from the reviewers, the revised formula results in fewer productivity points, which intuitively seems correct.
But if the writer's increased time results in even a modest reduction in the effort required by the reviewers (row 6), the writer is rewarded with more productivity points.
This should incentivize writers to balance quality and quantity.

.Pros and cons of using a ReviewerTime–WriterTime ratio
[cols="a,a"frame=ends]
|===
|Pros |Cons

|
* Requires very few inputs.
If writers and expert reviewers are already logging time, the only additional inputs are word count and complexity.

* Doesn't require gaining the cooperation of the editors and expert reviewers, which may be impractical.

* Does not require any subjective assessments by subject-matter experts or editors, avoiding one potential source of unfairness.

|
* Provides only an indirect measure of quality, and one that has not, to my knowledge, been tested.

* Might lack https://en.wikipedia.org/wiki/Face_validity[face validity^], that is, not be perceived to measure what it is meant to.

* Doesn't account for other documentation-related work such as editing, restructuring and so on.

* Does require tracking time per task.

|===

A reviewer's quality rating is a summary of the reviewer's judgment of the document in terms of a number of quality characteristics such as completeness, accuracy, clarity, style, and so forth.
As such, it reflects the document's intrinsic quality.
The claim I've made is that the WriterTime-ReviewerTime ratio is a proxy for this sort of measure of intrinsic quality.
In a later post I'll consider other ways of measuring quality (and by extension, productivity), including extrinsic measures such as user ratings.

I'd love to hear what you think.
Do any of the approaches outlined above seem workable, or are they just metrological madness?
