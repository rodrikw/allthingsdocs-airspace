---
title: "Productivity metrics for technical writers"
date: 2022-03-07T11:02:27+02:00
author: "Rodrik Wade"
image: "images/blog/dashboard.jpg"
image_caption: "<a href='https://unsplash.com/photos/MHIw0nSxCR4'>Untitled</a> by <a href='https://unsplash.com/@condorito1953'>Arie Wubben</a>, <a href='https://unsplash.com/license'>Unsplash licence</a>, cropped"
bg_image: "images/feature-bg.jpg"
categories: ["Managerial"]
tags: ["Metrics","Productivity"]
description: "A discussion of productivity metrics that can be applied to technical writers."
draft: true
type: "post"
---

Last year as part of project to detail a documentation strategy for a client, I was asked to include a section on metrics.
These included metrics for the following:

[horizontal]
Technical writer performance:: Metrics aimed at measuring the productivity of the technical writers.
Quality assurance:: Metrics aimed at measuring the quality of documentation.
These are valuable both for assessing the documentation itself and the technical writers responsible for it.
Customer feedback:: Metrics for assessing how users interact with online documentation as well as metrics for measuring users`' assessments of documentation.
Translation:: Metrics used to assess the productivity of translators as well as the metric most commonly used to charge for translation.

In this blog post I'll focus on the first of these.

I should say that I'm sceptical about the value of most metrics, especially productivity metrics.
Too often, metrics measure only what can be easily measured, not what it is actually important to measure.
In addition, metrics can have perverse effects, driving unwanted behaviour such as producing unnecessarily long documents in order to increase the writer's word count (similar to counting the number of lines of code that a developer produces).

As Tom Johnson points out in https://idratherbewriting.com/2012/03/02/technical-communication-metrics-what-should-you-track/[Technical Communication Metrics: What Should You Track?], productivity metrics need to take into account quality. He quotes Jack Barr and Stephanie Rosenbaum, who define productivity as follows:

[quote,"Barr, J.P. and Rosenbaum, S. (2003) Documentation and Training Productivity Benchmarks, Volume 50, No. 4, Nov 2003. p471."]
____
(Quality ร Quantity) รท Time = Productivity
____

Listed below are the metrics that I included for consideration by the client:

.Performance metrics for technical writers
[cols="3s,2a,5a",width="100%",options="header"]
|===
|Metric |Target |Description

|Average on-time delivery
|On time: 1 +
< one day late: 0.5 +
> one day late: 0

Target: > 0.9

|Was the documentation done (according to the definition of done) on time?
The definition of done is likely to specify that documentation is done when it is submitted to the repo and has been merged into the main branch.

|Average coverage completion
|Complete: 1 Incomplete: 0 Target: > 0.9
|An assessment (by the architect/{zwsp}product owner) as to whether the documentation submitted for inclusion in the build fully covers the sprint scope.

|Documentation points completed per time period
|This needs to be determined based on your experience and existing practice with developers.
|Unlike with translators, word count is a poor measure of technical writers`' productivity because it perversely promotes verbose documentation.
This is captured by a https://dilbert.com/strip/2007-11-28[Dilbert] cartoon from 2007 (I'm sure I didn't find this cartoon on my own but I'm afraid I can't recall where I came across the reference).

Counting documentation tickets completed by a writer over a period of time is another approach but this provides only a very rough measure because tickets vary enormously in the amount of work they involve.

Using points, as is common in a Scrum environment, allows for differing complexity to be taken into account (to some extent) and is thus fairer and more accurate.
If points are already used to estimate developers`' work, it would be beneficial to align the allocation of technical writer points with those of the developers.
Using points certainly adds to management complexity as it requires that the number of points be estimated for each task ahead of time.
However, over time experience should enable these point estimates to be made more quickly and accurately.


|Average documentation/{zwsp}review ratio
| >{nbsp}5:1
|Compares the amount of time spent by the technical writer in developing or updating documentation versus the amount of time required to review and/or edit the documentation.

Reviewers and editors should spend less than 20 percent of the development time for reviewing.
More than this suggests that the technical writer is "`offloading`" work onto the reviewers, that is, relying on reviewers to catch and correct mistakes.

In my view, this is a good summative metric, according with a manager's qualitative judgement: experienced technical writers will almost always have a greater documentation/{zwsp}review ratio than inexperienced ones.
I also like it because it builds in the quality aspect of productivity.

Nevertheless, care still needs to be taken in applying this metric.
If your best technical writer is consistently assigned the most troublesome projects, then their documentation/review ratio might be dragged down relative to an inexperienced writer who is only assigned easy UI descriptions.

Assuming that writers, developers, and architects all log time, this metric can be implemented by having two items per sprint to which documentation time is logged:

* Documentation update/{zwsp}development
* Documentation review

|Average review iterations required
|<{nbsp}3
|This is a similar metric to *Average documentation/{zwsp}review ratio*.
Although review/{zwsp}revision iterations are inevitable, they should be kept to a minimum.
Ideally, they should average less than three, that is, the reviewer should not normally need to review a document more than twice (initial review and approval).

For simplicity's sake, count each time a document is sent to the reviewer as one iteration, regardless of how in-depth the review is.

NOTE: This metric might have the unintended consequence of dissuading technical writers from sending their documentation for subsequent reviews.

|===

